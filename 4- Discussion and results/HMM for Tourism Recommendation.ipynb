{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alergia Algorithm applied to Trip Advisor data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation des packages nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction d'ouverture du csv.\n",
    "\n",
    "On drop un certain nombre de colonnes (souvent données géographiques car pas nécessaires pour l'analyse). On drop aussi les lignes ou des données sont manquantes (notamment quand la nationalité n'est pas renseignée)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ouverture(nom):\n",
    "    df = pd.read_csv(nom, sep = \",\" )\n",
    "    df = df.drop(columns = ['Unnamed: 0', 'gid_0', 'name_0', 'gid_1','name_1','gid_2', 'name_2', 'gid_3' ,'name_3', 'gid_4', 'name_4', 'gid_5', 'name_5', 'idplace'])\n",
    "    df['date_review'] = df['date_review'].replace('0000-00-00', np.nan)\n",
    "    df = df.dropna()\n",
    "    df['date_review'] = pd.to_datetime(df['date_review'], format = '%Y-%m-%d')\n",
    "    df = df[df.country != '-']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nom</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>id</th>\n",
       "      <th>idauteur</th>\n",
       "      <th>date_review</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L'Insoumise, France</td>\n",
       "      <td>48,835045</td>\n",
       "      <td>2,28915</td>\n",
       "      <td>328246480</td>\n",
       "      <td>AEC731BAB7B8CCA02944C293198E3FC6</td>\n",
       "      <td>2015-11-21</td>\n",
       "      <td>Italy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cinq Mondes Paris</td>\n",
       "      <td>48,871681</td>\n",
       "      <td>2,32899</td>\n",
       "      <td>331590265</td>\n",
       "      <td>26ABC72A68434467E4D20CEC4F1B3DC3</td>\n",
       "      <td>2015-12-09</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L'Envie, France</td>\n",
       "      <td>48,83414</td>\n",
       "      <td>2,317883</td>\n",
       "      <td>262837346</td>\n",
       "      <td>36B0E60FD21BFE2377C526209C3FF4D5</td>\n",
       "      <td>2015-03-31</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Le Ciel de Paris, France</td>\n",
       "      <td>48,84219</td>\n",
       "      <td>2,32198</td>\n",
       "      <td>266833774</td>\n",
       "      <td>72C94635808B0207A5A664DD646F3744</td>\n",
       "      <td>2015-04-20</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cafe Les Deux Magots, France</td>\n",
       "      <td>48,853973</td>\n",
       "      <td>2,333158</td>\n",
       "      <td>304112023</td>\n",
       "      <td>F007B2D8152512B20BB4EB3E7C5ACA2C</td>\n",
       "      <td>2015-08-27</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            nom   latitude longitude         id  \\\n",
       "0           L'Insoumise, France  48,835045   2,28915  328246480   \n",
       "1             Cinq Mondes Paris  48,871681   2,32899  331590265   \n",
       "2               L'Envie, France   48,83414  2,317883  262837346   \n",
       "3      Le Ciel de Paris, France   48,84219   2,32198  266833774   \n",
       "4  Cafe Les Deux Magots, France  48,853973  2,333158  304112023   \n",
       "\n",
       "                           idauteur date_review country  \n",
       "0  AEC731BAB7B8CCA02944C293198E3FC6  2015-11-21   Italy  \n",
       "1  26ABC72A68434467E4D20CEC4F1B3DC3  2015-12-09  France  \n",
       "2  36B0E60FD21BFE2377C526209C3FF4D5  2015-03-31  France  \n",
       "3  72C94635808B0207A5A664DD646F3744  2015-04-20  France  \n",
       "4  F007B2D8152512B20BB4EB3E7C5ACA2C  2015-08-27  Brazil  "
      ]
     },
     "execution_count": 1229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = ouverture('sample_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nombre de lieux différents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9420"
      ]
     },
     "execution_count": 1230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(df['nom'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nombre de reviews totales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56264"
      ]
     },
     "execution_count": 1231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection de l'année pour l'analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selection_annee(annee, df):\n",
    "    df = df[(df['date_review'].dt.year > annee - 1) & (df['date_review'].dt.year < annee + 1)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1233,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = selection_annee(2015, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nombre de reviews en 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56264"
      ]
     },
     "execution_count": 1234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection du nombre de lieux à traiter (les top k premiers)\n",
    "\n",
    "Pour se faire, on groupby le nom du lieu, auquel on applique la fonction count du nombre unique de review (représenté par la colonne id). On trie ensuite le résultat pour avoir le top de lieux représentés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k(k, df):\n",
    "    top_k = df.groupby('nom').count()['id'].sort_values(ascending = False)[0:k]\n",
    "    return top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1293,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_k = top_k(10, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nom\n",
       "Musée du Louvre                          1549\n",
       "Eiffel Tower                             1516\n",
       "Cathédrale Notre-Dame de Paris            749\n",
       "Arc de Triomphe                           711\n",
       "Jardin du Luxembourg                      589\n",
       "La Seine                                  478\n",
       "Musée d'Orsay                             413\n",
       "Basilique du Sacré-Cœur de Montmartre     367\n",
       "Champs-Élysées                            348\n",
       "Opéra Garnier                             333\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 1294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On peut maintenant réduire le DataFrame au top k de lieux "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_keep(df, top_k):\n",
    "    keep = pd.DataFrame(columns = df.columns)\n",
    "    for places in top_k.index:\n",
    "        keep = keep.append(df[df['nom'] == places])\n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1296,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = to_keep(df2, tp_k).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a le pourcentage que représente le top_K par rapport à la totalité des reviews de l'année choisie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12535546708374803"
      ]
     },
     "execution_count": 1297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df3.index)/len(df2.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Afin de créer les séquences, par soucis de clarté et de traitement, on va assigner un numéro à chaque lieux. Puis, le réduire à 1 digit via la colonne ascii.\n",
    "\n",
    "On pourra modifier plus tard l'algorithme Alergia afin qu'il puisse prendre en compte les doubles digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>nom</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>id</th>\n",
       "      <th>idauteur</th>\n",
       "      <th>date_review</th>\n",
       "      <th>country</th>\n",
       "      <th>Group_ID</th>\n",
       "      <th>ascii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>Musée du Louvre</td>\n",
       "      <td>48,861</td>\n",
       "      <td>2,335833</td>\n",
       "      <td>311501513</td>\n",
       "      <td>4B018C46529B92190134DEBD3D429BCD</td>\n",
       "      <td>2015-09-18</td>\n",
       "      <td>Italy</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76</td>\n",
       "      <td>Musée du Louvre</td>\n",
       "      <td>48,861</td>\n",
       "      <td>2,335833</td>\n",
       "      <td>288015394</td>\n",
       "      <td>294FB3D2C00277C2A2F563F2D4DA7A55</td>\n",
       "      <td>2015-07-11</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>117</td>\n",
       "      <td>Musée du Louvre</td>\n",
       "      <td>48,861</td>\n",
       "      <td>2,335833</td>\n",
       "      <td>316627370</td>\n",
       "      <td>39AE30708C84823D0A6DEFB86D7D2370</td>\n",
       "      <td>2015-10-06</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>143</td>\n",
       "      <td>Musée du Louvre</td>\n",
       "      <td>48,861</td>\n",
       "      <td>2,335833</td>\n",
       "      <td>277869091</td>\n",
       "      <td>2D508690DD4E5281BF88B235E1121A2C</td>\n",
       "      <td>2015-06-04</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>152</td>\n",
       "      <td>Musée du Louvre</td>\n",
       "      <td>48,861</td>\n",
       "      <td>2,335833</td>\n",
       "      <td>249606817</td>\n",
       "      <td>F6883072C58471A6A04D7FBFB9608DA2</td>\n",
       "      <td>2015-01-16</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index              nom latitude longitude         id  \\\n",
       "0      8  Musée du Louvre   48,861  2,335833  311501513   \n",
       "1     76  Musée du Louvre   48,861  2,335833  288015394   \n",
       "2    117  Musée du Louvre   48,861  2,335833  316627370   \n",
       "3    143  Musée du Louvre   48,861  2,335833  277869091   \n",
       "4    152  Musée du Louvre   48,861  2,335833  249606817   \n",
       "\n",
       "                           idauteur date_review    country  Group_ID ascii  \n",
       "0  4B018C46529B92190134DEBD3D429BCD  2015-09-18      Italy         8     8  \n",
       "1  294FB3D2C00277C2A2F563F2D4DA7A55  2015-07-11    Ireland         8     8  \n",
       "2  39AE30708C84823D0A6DEFB86D7D2370  2015-10-06  Argentina         8     8  \n",
       "3  2D508690DD4E5281BF88B235E1121A2C  2015-06-04     Turkey         8     8  \n",
       "4  F6883072C58471A6A04D7FBFB9608DA2  2015-01-16     Brazil         8     8  "
      ]
     },
     "execution_count": 1298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3['Group_ID'] = df3.groupby('nom').grouper.group_info[0]\n",
    "df3['ascii'] = df3.Group_ID.apply(lambda x: chr(x+48))\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6424"
      ]
     },
     "execution_count": 1299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(df3.idauteur)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asciiToName(df):\n",
    "    testdata = df[['ascii', 'nom']].values\n",
    "    return dict(list(set(tuple(x) for x in testdata)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1301,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert = asciiToName(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut ensuite avoir la correspondance des digit des séquences aux lieux "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'7': \"Musée d'Orsay\",\n",
       " '5': 'Jardin du Luxembourg',\n",
       " '0': 'Arc de Triomphe',\n",
       " '2': 'Cathédrale Notre-Dame de Paris',\n",
       " '3': 'Champs-Élysées',\n",
       " '9': 'Opéra Garnier',\n",
       " '4': 'Eiffel Tower',\n",
       " '6': 'La Seine',\n",
       " '8': 'Musée du Louvre',\n",
       " '1': 'Basilique du Sacré-Cœur de Montmartre'}"
      ]
     },
     "execution_count": 1302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On passe maintenant à la création des séquences pour chaque utilisateur. La particularité est qu'on va séparer les séquences en fonction du nombre de jours entre 2 photos.\n",
    "\n",
    "On commence par groupby les lieux dans l'ordre chronologique en fonction de l'id de l'auteur. Autrement dit, on va mettre dans une liste les lieux reviewés avec leur date de publication respective de chaque utilisateur. \n",
    "Puis, on va comparer le delta de temps entre la review i et la i + 1. Si c'est inférieur ou égal au paramètre qu'on souhaite, on ajoute l'élément à la séquence *a* . Sinon, on termine la séquence, et on sépare les deux reviews en 2 séquences *a* et *b*  distinctes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequencesv2(df, threshold_days):\n",
    "    df2 = df.sort_values('date_review', ascending = True).groupby('idauteur').apply(lambda x: x[['ascii','date_review']].values.tolist()).reset_index(name='col')\n",
    "    \n",
    "    df2['len'] = df2['col'].map(len)\n",
    "    df2 = df2[df2['len'] >= 2] \n",
    "\n",
    "    sequences_finales = [] #used to split patterns according to the threshold of days we want between 2 days\n",
    "            \n",
    "    for element in df2['col']:\n",
    "        sublist = ''\n",
    "        sublist2 = ''\n",
    "        for i in range(1, len(element)):\n",
    "            \n",
    "            diff = abs(element[i-1][1] - element[i][1]).days\n",
    "            \n",
    "            if(diff <= threshold_days): #IF THE PICTURE AND THE PREVIOUS ONE HAVE LESS THAN X DAYS \n",
    "                \n",
    "                if(i == 1): #if at the beginning of the sequence\n",
    "                    sublist += element[i-1][0] #we add the first element of the sequence\n",
    "        \n",
    "                sublist += element[i][0] #if not, we add the current one to the sublist\n",
    "                   \n",
    "            else: #IF THERE IS MORE THAN X DAYS :\n",
    "                \n",
    "                if(len(sublist) != 0): #if we stop a existing sequence, we add it\n",
    "                    sequences_finales.append(sublist)  \n",
    "                    sublist = ''\n",
    "                \n",
    "                if(i == 1): #if at the beginning of the sequence\n",
    "                    sublist += element[i-1][0] #we add the first one\n",
    "                    sequences_finales.append(sublist)  #then we cut the sub-sequence\n",
    "                    sublist ='' #and we reset the sub-sequence\n",
    "                    \n",
    "                sublist += element[i][0] #if not, we create another sub-sequence\n",
    "                \n",
    "                    \n",
    "        if(len(sublist) != 0): #at the end, we add the subsequence\n",
    "            sequences_finales.append(sublist)\n",
    "                    \n",
    "    lieux = df['nom'].unique() \n",
    "    \n",
    "    return lieux, sequences_finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1304,
   "metadata": {},
   "outputs": [],
   "source": [
    "lieux, sequences = create_sequencesv2(df3, 7) #utiliser 7 pour être en accord avec l'état de l'art"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on garde que les séquences de taille > 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutSeq(sequences):\n",
    "    seq2j = []\n",
    "    for s in sequences:\n",
    "        if len(s) >= 2:\n",
    "            seq2j.append(s)\n",
    "    return seq2j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1306,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2j = cutSeq(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averageLen(lst):\n",
    "    lengths = [len(i) for i in lst]\n",
    "    return 0 if len(lengths) == 0 else (float(sum(lengths)) / len(lengths)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "taille moyenne des séquences de touristes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0862533692722374"
      ]
     },
     "execution_count": 1308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averageLen(seq2j) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nombre totales de séquences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "371"
      ]
     },
     "execution_count": 1309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seq2j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "maxx = 0\n",
    "for s in seq2j:\n",
    "    if(len(s) > maxx):\n",
    "        maxx = len(s)\n",
    "print(maxx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici les 5 premières séquences que nous return l'algorithm. Nous n'avons accès à aucune donnée utilisateur, seulement à des séquence d'items (chaque symbole représente un lieu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['96', '10', '72', '19', '806']"
      ]
     },
     "execution_count": 1311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2j[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pour les séquences, ça part de 0 jusqu'a len. \n",
    "#### Tous les paramètres sont donc la position souhaitée - 1 (0 pour 1er élement, 1 pour 2ème, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSeqAtPosition(sequences, element, position):\n",
    "    result = []\n",
    "    for seq in sequences:\n",
    "        for i in range(len(seq)):\n",
    "            if(i == position):\n",
    "                if(seq[i] == element):\n",
    "                    result.append(seq)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1313,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourEiffel1 = getSeqAtPosition(seq2j[:1000], str(2), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1314,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourEiffel2 = getSeqAtPosition(seq2j[:1000], str(2), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1315,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourEiffel3 = getSeqAtPosition(seq2j[:1000], str(2), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1316,
   "metadata": {},
   "outputs": [],
   "source": [
    "notreDame1 = getSeqAtPosition(seq2j[:1000], str(1), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1317,
   "metadata": {},
   "outputs": [],
   "source": [
    "notreDame2 = getSeqAtPosition(seq2j[:1000], str(1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1318,
   "metadata": {},
   "outputs": [],
   "source": [
    "notreDame3 = getSeqAtPosition(seq2j[:1000], str(1), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1319,
   "metadata": {},
   "outputs": [],
   "source": [
    "laSeine1 = getSeqAtPosition(seq2j[:1000], str(4), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1320,
   "metadata": {},
   "outputs": [],
   "source": [
    "laSeine2 = getSeqAtPosition(seq2j[:1000], str(4), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1321,
   "metadata": {},
   "outputs": [],
   "source": [
    "laSeine3 = getSeqAtPosition(seq2j[:1000], str(4), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1322,
   "metadata": {},
   "outputs": [],
   "source": [
    "louvre1 = getSeqAtPosition(seq2j[:1000], str(5), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1355,
   "metadata": {},
   "outputs": [],
   "source": [
    "louvre2 = getSeqAtPosition(seq2j[:1000], str(5), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1356,
   "metadata": {},
   "outputs": [],
   "source": [
    "louvre3 = getSeqAtPosition(seq2j[:1000], str(5), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['56', '50', '52', '580', '58']\n",
      "['85', '65', '1583', '65', '75']\n",
      "['465', '605']\n"
     ]
    }
   ],
   "source": [
    "print(louvre1[:5])\n",
    "print(louvre2[:5])\n",
    "print(louvre3[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProbaPredecessor(sequences, element, position):\n",
    "    dic = dict.fromkeys(range(len(tp_k)), 0)\n",
    "    for seq in sequences:\n",
    "        key = int(seq[position - 1])\n",
    "        dic[key] += 1\n",
    "    res = {key: dic[key] / len(sequences) \n",
    "                        for key in dic.keys()}\n",
    "    values = sum(dic.values())\n",
    "    res = {key : dic[key] / values for key in dic.keys()}\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProbaSuccessor(sequences, element, position):\n",
    "    dic = dict.fromkeys(range(len(tp_k)), 0)\n",
    "    for seq in sequences:\n",
    "        if(len(seq) > position +1):\n",
    "            key = int(seq[position + 1])\n",
    "            dic[key] += 1\n",
    "    res = {key: dic[key] / len(sequences) \n",
    "                        for key in dic.keys()}\n",
    "    #values = sum(dic.values())\n",
    "    #res = {key : dic[key] / values for key in dic.keys()}\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.05,\n",
       " 1: 0.125,\n",
       " 2: 0.1,\n",
       " 3: 0.0,\n",
       " 4: 0.15,\n",
       " 5: 0.0,\n",
       " 6: 0.15,\n",
       " 7: 0.025,\n",
       " 8: 0.35,\n",
       " 9: 0.05}"
      ]
     },
     "execution_count": 1328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getProbaPredecessor(louvre2, 4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.15789473684210525,\n",
       " 1: 0.02631578947368421,\n",
       " 2: 0.07894736842105263,\n",
       " 3: 0.10526315789473684,\n",
       " 4: 0.07894736842105263,\n",
       " 5: 0.0,\n",
       " 6: 0.10526315789473684,\n",
       " 7: 0.0,\n",
       " 8: 0.3684210526315789,\n",
       " 9: 0.07894736842105263}"
      ]
     },
     "execution_count": 1358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getProbaSuccessor(louvre1, 5, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.0,\n",
       " 1: 0.0,\n",
       " 2: 0.05,\n",
       " 3: 0.0,\n",
       " 4: 0.0,\n",
       " 5: 0.0,\n",
       " 6: 0.0,\n",
       " 7: 0.0,\n",
       " 8: 0.05,\n",
       " 9: 0.0}"
      ]
     },
     "execution_count": 1359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getProbaSuccessor(louvre2, 5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.0,\n",
       " 1: 0.0,\n",
       " 2: 0.0,\n",
       " 3: 0.0,\n",
       " 4: 0.0,\n",
       " 5: 0.0,\n",
       " 6: 0.0,\n",
       " 7: 0.0,\n",
       " 8: 0.0,\n",
       " 9: 0.0}"
      ]
     },
     "execution_count": 1360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getProbaSuccessor(louvre3, 5, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the FPTA \n",
    "\n",
    "In order to be able to make our predictions, we first need to create our prefix tree, representing all the possible sequences read by the tree. \n",
    "\n",
    "We start by creating the *TrieNode* class. It represents a node of our tree. It takes as attribute:\n",
    "- a string of characters 'text', which represents the label of each node.\n",
    "- an 'is_word' boolean initialized to False. It becomes True if it has no children (i.e. if the word is finished).\n",
    "- a list of children, to which we initialize for all nodes the first element representing the number of words ending at this node (completes the boolean presented above)\n",
    "- an integer 'state', which represents the id of the node. It is unique for each node of the tree.\n",
    "- an 'is_displayed' boolean initialized to False. It is used when displaying the tree, when there are loops so that it is displayed only once.\n",
    "- a list of precedents, which grows as you go. All the precedents of each node are added to it. For the root node, we consider that its predecessor is itself.\n",
    "\n",
    "For the *PrefixTree* class, which represents the tree as such, we have:\n",
    "- a *TrieNode* which represents the root node.\n",
    "- class functions that we will detail below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class functions of the *PrefixTree*.\n",
    "\n",
    "#### Insert function:\n",
    "It allows us to insert a word (taken as a parameter), starting from a certain node (current parameter). To do so, we enumerate the word (each character is taken one by one with its corresponding index), then, if the character is not present in the list of children, we will create a node and then increment the indices. If there already exists a child node with the same character, we increment its frequency by 1. When the word is finished, we pass the boolean to True, and we increment the word counter by 1. \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/be/Trie_example.svg/1200px-Trie_example.svg.png\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1330,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrieNode:\n",
    "    _COUNTER_NODE = 1\n",
    "    def __init__(self, text = \"λ\"):\n",
    "        self.text = text\n",
    "        self.is_word = False\n",
    "        self.children = list()\n",
    "        self.children.append(['#', 0, self]) #1st element:SYMB 2nd element:FREQUENCY 3rd element:CHILDREN \n",
    "        self.state = TrieNode._COUNTER_NODE\n",
    "        TrieNode._COUNTER_NODE += 1\n",
    "        self.previous = []\n",
    "\n",
    "    def get_frequency(self): # Not counting the number of times a word ends \n",
    "        freq = 0\n",
    "        for child in self.children[1:]:\n",
    "            freq += child[1]\n",
    "        return freq\n",
    "    \n",
    "    def get_frequency2(self):\n",
    "        freq = 0\n",
    "        for child in self.children:\n",
    "            freq += child[1]\n",
    "        return freq\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'symb:{} state:{} -> children:{}'.format(self.text, self.state, self.children)\n",
    "\n",
    "class PrefixTree:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "        self.matrice_transition = pd.DataFrame()\n",
    "        self.matrice_emission = pd.DataFrame()\n",
    "\n",
    "    def insert(self, word, current = None):\n",
    "        '''\n",
    "        Creates a given word in the trie\n",
    "        '''\n",
    "        if not current: \n",
    "            current = self.root\n",
    "            pre = self.root\n",
    "            \n",
    "        for i, char in enumerate(word):\n",
    "            \n",
    "            if char not in [x[0] for x in current.children]:\n",
    "                prefix = word[0:i+1]\n",
    "                current.children.append([char, 1, TrieNode(prefix)])\n",
    "                \n",
    "                if pre not in current.previous:\n",
    "                    current.previous.append(pre)\n",
    "                    \n",
    "                pre = current\n",
    "                current = current.children[-1][2]\n",
    "\n",
    "            else:\n",
    "                for child in current.children:\n",
    "                    if child[0] == char:\n",
    "                        child[1] += 1\n",
    "                        \n",
    "                        if pre not in current.previous:\n",
    "                            current.previous.append(pre)\n",
    "                            \n",
    "                        pre = current\n",
    "                        current = child[2]\n",
    "\n",
    "        current.is_word = True   \n",
    "        current.children[0][1] += 1\n",
    "        \n",
    "    def starts_with(self, prefix, current = None):\n",
    "        '''\n",
    "        Returns a list of all words beginning with the given prefix, or\n",
    "        an empty list if no words begin with that prefix.\n",
    "        '''\n",
    "        words = list()\n",
    "        if not current:\n",
    "            current = self.root\n",
    "        for char in prefix:\n",
    "            if char not in [x[0] for x in current.children]:\n",
    "                return list()\n",
    "            for child in current.children:\n",
    "                  if child[0] == char:\n",
    "                    current = child[2]\n",
    "        self.__child_words_for(current, words)\n",
    "        return words\n",
    "    \n",
    "    def __child_words_for(self, node, words):\n",
    "        '''\n",
    "        Private helper function. Cycles through all children\n",
    "        of node recursively, adding them to words if they\n",
    "        constitute whole words (as opposed to merely prefixes).\n",
    "        '''\n",
    "        if node.is_word:\n",
    "            words.append(node.text)\n",
    "        for child in node.children[1:]:\n",
    "            self.__child_words_for(child[2], words)\n",
    "    \n",
    "    def size(self, current = None, visited = None):\n",
    "        '''\n",
    "        Returns the size of this prefix tree, defined\n",
    "        as the total number of nodes in the tree.\n",
    "        '''\n",
    "        # By default, get the size of the whole trie, starting at the root\n",
    "        if not current:\n",
    "            current = self.root\n",
    "        \n",
    "        if not visited:\n",
    "            visited = []\n",
    "            visited.append(current)\n",
    "            \n",
    "        count = 1\n",
    "        \n",
    "        for child in current.children[1:]:\n",
    "            if(child[2] not in visited):\n",
    "                visited.append(child[2])\n",
    "                count += self.size(child[2], visited)\n",
    "                \n",
    "        return count\n",
    "\n",
    "    def leaf(self, current = None, visited = None):\n",
    "        if not current: \n",
    "            current = self.root\n",
    "            \n",
    "        if not visited:\n",
    "            visited = []\n",
    "            \n",
    "        count = 0\n",
    "        tmp = 0\n",
    "        \n",
    "        self.__child_words_for(current, count)        \n",
    "        return count\n",
    "    \n",
    "    def __leaves_for(self, current, count, visited):\n",
    "        if current.is_word == True:\n",
    "            count += 1\n",
    "        for child in current.children[1:]:\n",
    "            if(child[2] not in visited):\n",
    "                visited.append(child[2])\n",
    "                self.__leaves_for(child[2], count, visited)\n",
    "    \n",
    "    def transform2proba(self, current = None):\n",
    "        if not current:\n",
    "            current = self.root\n",
    "            \n",
    "        freq = current.get_frequency2()\n",
    "        for child in current.children:\n",
    "            if(isinstance(child[1], int) == False):\n",
    "                return 0\n",
    "            else: \n",
    "                child[1] = child[1]/freq\n",
    "                if child[2].state != current.state:\n",
    "                    current = child[2]    \n",
    "                    self.transform2proba(current)\n",
    "    \n",
    "    def group_transition(self, node):\n",
    "        result = []\n",
    "        for child in node.children:\n",
    "            list_child = [x for x in node.children if x[2].state == child[2].state]\n",
    "            if list_child not in result:\n",
    "                result.append(list_child)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def transform_to_HMMT(self, current = None, visited = None):\n",
    "        if not current:\n",
    "            current = self.root\n",
    "\n",
    "        if not visited:\n",
    "            visited = []\n",
    "            visited.append(current)\n",
    "\n",
    "        freq = current.get_frequency2()\n",
    "\n",
    "        for child in current.children:\n",
    "            if(len(child) <= 3):\n",
    "                trans = self.group_transition(current)\n",
    "\n",
    "                for sub in trans:\n",
    "                    sum_ = 0\n",
    "                    for node in sub:\n",
    "                        sum_ += node[1]\n",
    "                    for node in sub:\n",
    "                        node[1] /= sum_\n",
    "                        node.append(sum_)              \n",
    "\n",
    "        for child in current.children[1:]:\n",
    "            if child[2] not in visited:  \n",
    "                visited.append(current) \n",
    "                self.transform_to_HMMT(child[2], visited)   \n",
    "                \n",
    "    def create_matrix_transition(self, current = None, visited = None):\n",
    "        if not current:\n",
    "            current = self.root\n",
    "        \n",
    "        if not visited:\n",
    "            visited = []   \n",
    "            \n",
    "        for i in range(len(current.children)):\n",
    "            listt = self.group_transition(current)\n",
    "            for c in listt:\n",
    "                self.matrice_transition.loc[current.text, c[0][2].text] = round(c[0][3], 2)\n",
    "            visited.append(current)\n",
    "                \n",
    "        for i in range(len(current.children)):\n",
    "            if(current.children[i][2] not in visited):\n",
    "                self.create_matrix_transition(current.children[i][2], visited) \n",
    "                \n",
    "        self.matrice_transition = self.matrice_transition.fillna(0).astype('float')\n",
    "        #self.matrice_transition = self.matrice_transition.to_numpy(dtype = 'float')\n",
    "        \n",
    "    def create_matrix_emission(self, current = None, visited = None):\n",
    "        if not current:\n",
    "            current = self.root\n",
    "        \n",
    "        if not visited:\n",
    "            visited = []\n",
    "            \n",
    "        for i in range(len(current.children)):\n",
    "            self.matrice_emission.loc[current.text, current.children[i][0]] = round(current.children[i][1], 2)\n",
    "            visited.append(current)\n",
    "        \n",
    "        for i in range(len(current.children)):\n",
    "            if(current.children[i][2] not in visited):\n",
    "                self.create_matrix_emission(current.children[i][2], visited) \n",
    "        \n",
    "                \n",
    "        self.matrice_emission = self.matrice_emission.fillna(0).astype('float')\n",
    "        self.matrice_emission = self.matrice_emission.reindex(sorted(self.matrice_emission.columns), axis=1)\n",
    "        #self.matrice_emission = self.matrice_emission.to_numpy(dtype = 'float')\n",
    "        \n",
    "    def viterbi_path(self, observations, prior = None, scaled=True, ret_loglik=False):\n",
    "        '''Finds the most-probable (Viterbi) path through the HMM state trellis\n",
    "        Notation:\n",
    "            Z[t] := Observation at time t\n",
    "            Q[t] := Hidden state at time t\n",
    "        Inputs:\n",
    "            prior: np.array(num_hid)\n",
    "                prior[i] := Pr(Q[0] == i)\n",
    "            transmat: np.ndarray((num_hid,num_hid))\n",
    "                transmat[i,j] := Pr(Q[t+1] == j | Q[t] == i)\n",
    "            obslik: np.ndarray((num_hid,num_obs))\n",
    "                obslik[i,t] := Pr(Z[t] | Q[t] == i)\n",
    "            scaled: bool\n",
    "                whether or not to normalize the probability trellis along the way\n",
    "                doing so prevents underflow by repeated multiplications of probabilities\n",
    "            ret_loglik: bool\n",
    "                whether or not to return the log-likelihood of the best path\n",
    "        Outputs:\n",
    "            path: np.array(num_obs)\n",
    "                path[t] := Q[t]\n",
    "        '''\n",
    "        obslik = np.array([self.matrice_emission.to_numpy()[:, z] for z in observations]).T\n",
    "        num_hid = obslik.shape[0] # number of hidden states\n",
    "        num_obs = obslik.shape[1] # number of observations (not observation *states*)\n",
    "        \n",
    "        if not prior:\n",
    "            prior = np.zeros(self.matrice_transition.to_numpy().shape[0])\n",
    "            prior[0] = 1\n",
    "\n",
    "        # trellis_prob[i,t] := Pr((best sequence of length t-1 goes to state i), Z[1:(t+1)])\n",
    "        trellis_prob = np.zeros((num_hid,num_obs))\n",
    "\n",
    "        # trellis_state[i,t] := best predecessor state given that we ended up in state i at t\n",
    "        trellis_state = np.zeros((num_hid,num_obs), dtype=int) # int because its elements will be used as indicies\n",
    "\n",
    "        path = np.zeros(num_obs, dtype=int) # int because its elements will be used as indicies\n",
    "\n",
    "        trellis_prob[:,0] = np.dot(prior, obslik[:,0]) # element-wise mult\n",
    "\n",
    "        if scaled:\n",
    "            scale = np.ones(num_obs) # only instantiated if necessary to save memory\n",
    "            scale[0] = 1.0 / np.sum(trellis_prob[:,0])\n",
    "            trellis_prob[:,0] *= scale[0]\n",
    "\n",
    "        trellis_state[:,0] = 0 # arbitrary value since t == 0 has no predecessor\n",
    "\n",
    "        for t in range(1, num_obs):\n",
    "            for j in range(num_hid):\n",
    "                trans_probs = trellis_prob[:, t - 1] * self.matrice_transition.to_numpy()[:, j] # element-wise mult\n",
    "                trellis_state[j,t] = trans_probs.argmax()\n",
    "                trellis_prob[j,t] = trans_probs[trellis_state[j,t]] # max of trans_probs\n",
    "                trellis_prob[j,t] *= obslik[j,t]\n",
    "            if scaled:\n",
    "                scale[t] = 1.0 / np.sum(trellis_prob[:,t])\n",
    "                trellis_prob[:,t] *= scale[t]\n",
    "\n",
    "        path[-1] = trellis_prob[:,-1].argmax()\n",
    "        for t in range(num_obs-2, -1, -1):\n",
    "            path[t] = trellis_state[(path[t+1]), t+1]\n",
    "\n",
    "        if not ret_loglik:\n",
    "            return path\n",
    "        else:\n",
    "            if scaled:\n",
    "                loglik = -np.sum(np.log(scale))\n",
    "            else:\n",
    "                p = trellis_prob[path[-1], -1]\n",
    "                loglik = p\n",
    "            return path, loglik\n",
    "    \n",
    "    def find_node(self, text, current = None, visited = None):\n",
    "        if not current:\n",
    "            current = self.root\n",
    "            \n",
    "        if not visited:\n",
    "            visited = []\n",
    "        \n",
    "        if current.text == text:\n",
    "            return current\n",
    "        \n",
    "        for child in current.children[1:]:\n",
    "            if(child[2] not in visited):\n",
    "                current = child[2]\n",
    "                visited.append(current)\n",
    "                found = self.find_node(text, current = current, visited = visited)\n",
    "                if found: \n",
    "                    return found\n",
    "    \n",
    "    def prediction(self, seq, L):\n",
    "        S = dict()\n",
    "        path = self.viterbi_path(seq)\n",
    "        current = self.find_node(text = self.matrice_transition.index[path[-1] - 1])\n",
    "        return self.suffixes(current, 1, S, '', L)\n",
    "    \n",
    "    def suffixes(self, current, probability, S, suffix, L):\n",
    "        if L == 0:\n",
    "            return S\n",
    "        \n",
    "        else:\n",
    "            for child in current.children[1:]:\n",
    "                suffix += child[0]\n",
    "                probability = probability * child[1] * child[3]\n",
    "                S[suffix] = probability\n",
    "                found = self.suffixes(child[2], probability, S, suffix, L-1)\n",
    "                if found:\n",
    "                    return found\n",
    "                          \n",
    "    def forward(self, V, a, b, initial_distribution):\n",
    "        alpha = np.zeros((V.shape[0], a.shape[0]))\n",
    "        alpha[0, :] = initial_distribution * b[:, V[0] + 1]\n",
    "        #print(alpha)\n",
    "        for t in range(1, V.shape[0]):\n",
    "            for j in range(a.shape[0]):\n",
    "                # Matrix Computation Steps\n",
    "                #                  ((1x2) . (1x2))      *     (1)\n",
    "                #                        (1)            *     (1)\n",
    "                alpha[t, j] = alpha[t - 1].dot(a[:, j]) * b[j, V[t] + 1]\n",
    "            \n",
    "        return alpha\n",
    " \n",
    "    def backward(self, V, a, b):\n",
    "        beta = np.zeros((V.shape[0], a.shape[0]))\n",
    "\n",
    "        # setting beta(T) = 1\n",
    "        beta[V.shape[0] - 1] = np.ones((a.shape[0]))\n",
    "\n",
    "        # Loop in backward way from T-1 to\n",
    "        # Due to python indexing the actual loop will be T-2 to 0\n",
    "        for t in range(V.shape[0] - 2, -1, -1):\n",
    "            for j in range(a.shape[0]):\n",
    "                beta[t, j] = (beta[t + 1] * b[:, V[t + 1] + 1]).dot(a[j, :])\n",
    "        \n",
    "        return beta\n",
    " \n",
    "    def baum_welch(self, obs, initial_distribution = None, n_iter = 100):\n",
    "        \n",
    "        a = self.matrice_transition.to_numpy()\n",
    "        b = self.matrice_emission.to_numpy()\n",
    "        \n",
    "        \n",
    "        if not initial_distribution:\n",
    "            initial_distribution = np.full(self.matrice_transition.to_numpy().shape[0], 1/self.matrice_transition.to_numpy().shape[0])\n",
    "            #initial_distribution[0] = 1\n",
    "        \n",
    "        M = a.shape[0]\n",
    "        T = len(obs)\n",
    "\n",
    "        for n in range(n_iter):\n",
    "            alpha = self.forward(obs, a, b, initial_distribution)\n",
    "            beta = self.backward(obs, a, b)\n",
    "\n",
    "            xi = np.zeros((M, M, T - 1))\n",
    "            for t in range(T - 1):\n",
    "                denominator = np.dot(np.dot(alpha[t, :].T, a) * b[:, obs[t + 1] + 1].T, beta[t + 1, :])\n",
    "                for i in range(M):\n",
    "                    numerator = alpha[t, i] * a[i, :] * b[:, obs[t + 1] + 1].T * beta[t + 1, :].T\n",
    "                    xi[i, :, t] = numerator / denominator\n",
    "\n",
    "            gamma = np.sum(xi, axis=1) \n",
    "            a = np.sum(xi, 2) / np.sum(gamma, axis=1).reshape((-1, 1))\n",
    "\n",
    "            # Add additional T'th element in gamma\n",
    "            gamma = np.hstack((gamma, np.sum(xi[:, :, T - 2], axis=0).reshape((-1, 1))))\n",
    "\n",
    "            K = b.shape[1]\n",
    "            denominator = np.sum(gamma, axis=1)\n",
    "            for l in range(K):\n",
    "                b[:, l] = np.sum(gamma[:, obs == l + 1], axis=1)\n",
    "\n",
    "            b = np.divide(b, denominator.reshape((-1, 1)))\n",
    "            \n",
    "        tmp1 = self.matrice_transition\n",
    "        tmp2 = self.matrice_emission\n",
    "        \n",
    "        self.matrice_transition = pd.DataFrame(a, index = tmp1.index, columns = tmp1.columns)\n",
    "        self.matrice_emission = pd.DataFrame(b, index = tmp2.index, columns = tmp2.columns)\n",
    "    \n",
    "    def display(self):\n",
    "        '''\n",
    "        Prints the contents of this prefix tree.\n",
    "        '''\n",
    "        print('====================================================================================')\n",
    "        self.__displayHelper(self.root)\n",
    "        print('====================================================================================\\n')\n",
    "    \n",
    "    def __displayHelper(self, current, visited = None):\n",
    "        '''\n",
    "        Private helper for printing the contents of this prefix tree.\n",
    "        '''\n",
    "        if not visited: \n",
    "            visited = []\n",
    "            visited.append(current)\n",
    "        \n",
    "        print(current, \"\\n\")\n",
    "        \n",
    "        for child in current.children[1:]:\n",
    "            if child[2] not in visited:\n",
    "                visited.append(current)\n",
    "                self.__displayHelper(child[2], visited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaa', 'aaba', 'ababa', 'bb', 'bbaaa', 'cb']\n"
     ]
    }
   ],
   "source": [
    "seq = ['aaa', 'aaba', 'ababa', 'bb', 'cb', 'bbaaa']\n",
    "seq.sort()\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1332,
   "metadata": {},
   "outputs": [],
   "source": [
    "trie = PrefixTree()\n",
    "for seqq in seq:\n",
    "    trie.insert(seqq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the DFA with the FPTA:\n",
    "\n",
    "To transform a prefix tree into a frequency automaton (and thus probabilistic), a state-merging algorithm is used. Currently, the best one has been created by Colin de la Higuera. It is implemented from scratch below:\n",
    "\n",
    "First of all, we have in parameters the sequences and an alpha parameter [0, 1]. \n",
    "We create the tree then we initialize 2 lists: red & blue: \n",
    "- Red stores all nodes that have already been defined as representative nodes and will be included in the final output model.\n",
    "- Blue stores nodes that have not yet been tested.\n",
    "\n",
    "The purpose of the algorithm is to keep only the red states. At the beginning,RED contains only the root node, whileBlue contains the immediate successor nodes of the initial node: the child nodes of the root. When executing the external loop of the algorithm, the first qb node in BLUE is chosen. If there is a qr node in RED compatible with qb, then qb and its successor nodes are merged with qr and the corresponding successor nodes of qr, respectively, as described in the Merge&Fold part. If qb is not compatible with any of the states of RED, it will be promoted, i.e. it will switch to RED, and its respective children will switch to BLUE.\n",
    "\n",
    "#### Merge&Fold operation:\n",
    "Given the DFA, the merge operation takes 2 states q and q′, where q is a RED state and q′ is a BLUE state compatible with the Hoeffding test. Merge operation consists of 2 steps. First, if q′est is a final state, then q becomes one as well (if it was not already the case) and the number of sequences ending in q′ is added to the number of sequences ending in q. Second, incoming arcs at q′sont redirected to q. If such arcs already exist, the number of passing sequences in each incoming arc is added to the existing arc. The Fold operation is a recursive function consisting in merging the successor nodes q′ into q and the corresponding successor nodes of q respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Original_Alergia(sequences, alpha):\n",
    "    '''\n",
    "    Original version of the Alergia Algorithm\n",
    "    \n",
    "    Arguments: \n",
    "    sequences -- the sequences to build the Trie and then convert it in a PFA\n",
    "    alpha -- an int between 0 and 1. It represents a parameter of the Hoeffding bounds. \n",
    "    \n",
    "    Return:\n",
    "    A DPA according to the sequences we have\n",
    "    '''\n",
    "    print('Starting building corresponding Trie \\n')\n",
    "    trie = PrefixTree()\n",
    "    trie.alphabet = list({l for word in sequences for l in word})\n",
    "    for subseq in sequences:\n",
    "        trie.insert(subseq)\n",
    "    print(trie.size())\n",
    "    #print(trie.leaf())\n",
    "    #initial = trie.size()\n",
    "    red = []\n",
    "    red.append(trie.root)\n",
    "    blue = []\n",
    "    for x in trie.root.children[1:]:\n",
    "        if(x[2] not in blue):\n",
    "            blue.append(x[2])\n",
    "    t0 = 0.5\n",
    "    \n",
    "    print('Running Alergia on trie \\n')\n",
    "    while(len(blue) > 0):\n",
    "        qb = blue[0]\n",
    "        blue.remove(qb)\n",
    "        promote = True\n",
    "        if(qb.get_frequency() >= t0):\n",
    "            for qr in red:\n",
    "                if(Original_AlergiaCompatible(trie, qr, qb, alpha)):\n",
    "                    #print('Merge accepted...')\n",
    "                    #print(qr)\n",
    "                    #print(qb)\n",
    "                    trie = Original_MergeFold(trie, qr, qb)\n",
    "                    promote = False\n",
    "                    for child in qr.children[1:]:    \n",
    "                        if(child[2] not in blue and child[2].state != qr.state and child[2] not in red):\n",
    "                            blue.append(child[2])\n",
    "                    break\n",
    "        \n",
    "            if(promote == True):\n",
    "                #print('No merge possible...')\n",
    "                red.append(qb)\n",
    "                for child in qb.children[1:]:\n",
    "                    if(child[2] not in blue and child[2] not in red):\n",
    "                        blue.append(child[2])\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    trie.transform2proba()\n",
    "    print('Algo done')\n",
    "    return trie\n",
    "\n",
    "def Original_AlergiaCompatible(trie, qr, qb, alpha):\n",
    "    '''\n",
    "    Sub-method of the Alergia main one. It tells us if 2 nodes (a red and a blue) are compatible\n",
    "    \n",
    "    Arguments:\n",
    "    trie -- the current trie we are working on\n",
    "    qr -- a red node \n",
    "    qb -- a blue node\n",
    "    alpha -- parameter alpha mentionned above\n",
    "    \n",
    "    Return:\n",
    "    a boolean saying whether or not the 2 nodes in arguments are compatible\n",
    "    '''\n",
    "    correct = True\n",
    "    if(Original_AlergiaTest(qr.children[0][1], qr.get_frequency(), qb.children[0][1], qb.get_frequency(), alpha) == False):\n",
    "        correct = False\n",
    "    \n",
    "    for a in trie.alphabet:\n",
    "        for children_r in qr.children:\n",
    "            for children_b in qb.children:\n",
    "                if(a == children_r[1] and a == children_b[1]):\n",
    "                    if(Original_AlergiaTest(children_r[2], children_r.get_frequency(), children_b[2], children_b.get_frequency(), alpha) == False):\n",
    "                        correct = False\n",
    "    return correct\n",
    "  \n",
    "def Original_AlergiaTest(f1, n1, f2, n2, alpha):\n",
    "    '''\n",
    "    Sub-method of the Alergia compatible method. It computes the hoeffding bounds and compare it to the gamma threshold\n",
    "    \n",
    "    Arguments:\n",
    "    f1 -- incoming frequency of the node 1\n",
    "    n1 -- number of times a word ends in node 1\n",
    "    f2 -- incoming frequency of the node 2\n",
    "    n2 -- number of times a word ends in node 2\n",
    "    1 & 2 refers to the nodes we want to compare (i.e a node Red and a node Blue)\n",
    "    alpha -- same parameter alpha as mentionned\n",
    "    \n",
    "    Return:\n",
    "    True if gamma < hoeffding (then, the nodes are compatible)\n",
    "    False if not\n",
    "    '''\n",
    "    gamma = math.fabs(f1/n1 - f2/n2)\n",
    "    root = math.sqrt(1/(2*math.log(2/alpha)))\n",
    "    summ = (1/math.sqrt(n1)) + (1/math.sqrt(n2))\n",
    "    hoeffding = root * summ\n",
    "    return gamma < hoeffding\n",
    "\n",
    "def Original_MergeFold(trie, qr, qb): \n",
    "    '''\n",
    "    MERGE OPERATION: \n",
    "    To merge a BLUE node into a RED node, all the ingoing links of BLUE node are redirected to the RED node. \n",
    "    If the RED node already has an ingoing link with the same item attribute as a redirected link, then only the frequencies are added. \n",
    "    Next, the BLUE node attribute is added to the RED node attribute.\n",
    "    \n",
    "    FOLD OPERATION:\n",
    "    When a BLUE node merges into a RED node, all children of BLUE node are recursively merged into children of the RED node with the same item attribute link.\n",
    "    If the link with an item attribute doesn't exist in the RED node, a new child is inserted.\n",
    "    \n",
    "    Arguments:\n",
    "    trie -- the current Trie we are working on\n",
    "    qr -- the red node \n",
    "    qb -- the blue node to merge\n",
    "    '''\n",
    "    q = qb.previous[0]\n",
    "    \n",
    "    if qb.text[-1] not in qr.text:\n",
    "        qr.text = qr.text+ ', ' + qb.text[-1]\n",
    "        \n",
    "    for child in q.children:\n",
    "        if(child[2].state == qb.state):\n",
    "            child[2] = qr \n",
    "            \n",
    "    qr.children[0][1] += qb.children[0][1]\n",
    "    \n",
    "    words = trie.starts_with('', current = qb)\n",
    "    \n",
    "    for word in words:\n",
    "        trie.insert(word)\n",
    "        \n",
    "    return trie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting building corresponding Trie \n",
      "\n",
      "115\n",
      "Running Alergia on trie \n",
      "\n",
      "Algo done\n",
      "Wall time: 5 ms\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "%time dfa = Original_Alergia(seq2j, 0.5)\n",
    "print(dfa.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1348,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa.transform_to_HMMT()\n",
    "dfa.create_matrix_transition()\n",
    "dfa.create_matrix_emission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Alergia(sequences, alpha):\n",
    "    '''\n",
    "    Our version of the Alergia algorithm. It tries to recursively merge all the pairs of nodes, from the root to the leaves.\n",
    "    Unlike the original version, the difference is that it does not test the compatibility of children of these pairs of nodes.\n",
    "    \n",
    "    Arguments: \n",
    "    sequences -- the sequences to build the Trie and then convert it in a PFA\n",
    "    alpha -- an int between 0 and 1. It represents a parameter of the Hoeffding bounds. \n",
    "    \n",
    "    Return:\n",
    "    A DPA according to the sequences we have\n",
    "    '''\n",
    "    \n",
    "    print('Starting building corresponding Trie \\n')\n",
    "    \n",
    "    trie = PrefixTree()\n",
    "    \n",
    "    trie.alphabet = list({l for word in sequences for l in word})\n",
    "    \n",
    "    for subseq in sequences:\n",
    "        trie.insert(subseq)\n",
    "        \n",
    "    #print(trie.size())\n",
    "    #print(trie.leaf())\n",
    "    #initial = trie.size()\n",
    "    red = []\n",
    "    \n",
    "    red.append(trie.root)\n",
    "    \n",
    "    blue = []\n",
    "    \n",
    "    for x in trie.root.children[1:]:\n",
    "        if(x[2] not in blue):\n",
    "            blue.append(x[2])\n",
    "            \n",
    "    t0 = 0.5\n",
    "    \n",
    "    print('Running Alergia on trie \\n')\n",
    "    \n",
    "    while(len(blue) > 0):\n",
    "        qb = blue[0]\n",
    "        blue.remove(qb)\n",
    "        promote = True\n",
    "        if(qb.get_frequency() >= t0):\n",
    "            for qr in red:\n",
    "                if(AlergiaCompatible(trie, qr, qb, alpha)):\n",
    "                    #print('Merge accepted...')\n",
    "                    #print(qr)\n",
    "                    #print(qb)\n",
    "                    trie = MergeFold(trie, qr, qb)\n",
    "                    promote = False\n",
    "                    for child in qr.children[1:]:    \n",
    "                        if(child[2] not in blue and child[2].state != qr.state and child[2] not in red):\n",
    "                            blue.append(child[2])\n",
    "                    break\n",
    "        \n",
    "            if(promote == True):\n",
    "                #print('No merge possible...')\n",
    "                red.append(qb)\n",
    "                for child in qb.children[1:]:\n",
    "                    if(child[2] not in blue and child[2] not in red):\n",
    "                        blue.append(child[2])\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    trie.transform2proba()\n",
    "    print('Algo done')\n",
    "    return trie\n",
    "\n",
    "def AlergiaCompatible(trie, qr, qb, alpha):\n",
    "    '''\n",
    "    Sub-method of the Alergia main one. It tells us if 2 nodes (a red and a blue) are compatible\n",
    "    \n",
    "    Arguments:\n",
    "    trie -- the current trie we are working on\n",
    "    qr -- a red node \n",
    "    qb -- a blue node\n",
    "    alpha -- parameter alpha mentionned above\n",
    "    \n",
    "    Return:\n",
    "    a boolean saying whether or not the 2 nodes in arguments are compatible\n",
    "    '''\n",
    "    correct = True\n",
    "    \n",
    "    if(AlergiaTest(qr.children[0][1], qr.get_frequency(), qb.children[0][1], qb.get_frequency(), alpha) == False):\n",
    "        correct = False\n",
    "    \n",
    "    return correct\n",
    "  \n",
    "def AlergiaTest(f1, n1, f2, n2, alpha):\n",
    "    '''\n",
    "    Sub-method of the Alergia compatible method. It computes the hoeffding bounds and compare it to the gamma threshold\n",
    "    \n",
    "    Arguments:\n",
    "    f1 -- incoming frequency of the node 1\n",
    "    n1 -- number of times a word ends in node 1\n",
    "    f2 -- incoming frequency of the node 2\n",
    "    n2 -- number of times a word ends in node 2\n",
    "    1 & 2 refers to the nodes we want to compare (i.e a node Red and a node Blue)\n",
    "    alpha -- same parameter alpha as mentionned\n",
    "    \n",
    "    Return:\n",
    "    True if gamma < hoeffding (then, the nodes are compatible)\n",
    "    False if not\n",
    "    '''\n",
    "    gamma = math.fabs(f1/n1 - f2/n2)\n",
    "    root = math.sqrt(1/(2*math.log(2/alpha)))\n",
    "    summ = (1/math.sqrt(n1)) + (1/math.sqrt(n2))\n",
    "    hoeffding = root * summ\n",
    "    return gamma < hoeffding\n",
    "\n",
    "def MergeFold(trie, qr, qb): \n",
    "    '''\n",
    "    MERGE OPERATION: \n",
    "    To merge a BLUE node into a RED node, all the ingoing links of BLUE node are redirected to the RED node. \n",
    "    If the RED node already has an ingoing link with the same item attribute as a redirected link, then only the frequencies are added. \n",
    "    Next, the BLUE node attribute is added to the RED node attribute.\n",
    "    \n",
    "    FOLD OPERATION:\n",
    "    When a BLUE node merges into a RED node, all children of BLUE node are recursively merged into children of the RED node with the same item attribute link.\n",
    "    If the link with an item attribute doesn't exist in the RED node, a new child is inserted.\n",
    "    \n",
    "    Arguments:\n",
    "    trie -- the current Trie we are working on\n",
    "    qr -- the red node \n",
    "    qb -- the blue node to merge\n",
    "    '''\n",
    "    q = qb.previous[0]\n",
    "    \n",
    "    if qb.text[-1] not in qr.text:\n",
    "        qr.text = qr.text+ ', ' + qb.text[-1]\n",
    "        \n",
    "    for child in q.children:\n",
    "        if(child[2].state == qb.state):\n",
    "            child[2] = qr \n",
    "            \n",
    "    qr.children[0][1] += qb.children[0][1]\n",
    "    \n",
    "    words = trie.starts_with('', current = qb)\n",
    "    \n",
    "    for word in words:\n",
    "        trie.insert(word)\n",
    "        \n",
    "    return trie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting building corresponding Trie \n",
      "\n",
      "Running Alergia on trie \n",
      "\n",
      "Algo done\n",
      "Wall time: 3.98 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 1349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time dfa_2 = Alergia(seq2j, 1/125)\n",
    "dfa_2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1353,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa_2.transform_to_HMMT()\n",
    "dfa_2.create_matrix_transition()\n",
    "dfa_2.create_matrix_emission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================\n",
      "symb:λ, 9, 1, 7, 8, 3, 0 state:256 -> children:[['#', 0.07738095238095238, <__main__.TrieNode object at 0x0000023F906C1988>, 0.6086956521739131], ['9', 0.0744047619047619, <__main__.TrieNode object at 0x0000023F906C1988>, 0.6086956521739131], ['1', 0.11309523809523808, <__main__.TrieNode object at 0x0000023F906C1988>, 0.6086956521739131], ['7', 0.05654761904761904, <__main__.TrieNode object at 0x0000023F906C1988>, 0.6086956521739131], ['8', 0.33035714285714285, <__main__.TrieNode object at 0x0000023F906C1988>, 0.6086956521739131], ['3', 0.10416666666666667, <__main__.TrieNode object at 0x0000023F906C1988>, 0.6086956521739131], ['0', 0.244047619047619, <__main__.TrieNode object at 0x0000023F906C1988>, 0.6086956521739131], ['2', 0.24550898203592814, <__main__.TrieNode object at 0x0000023F906CEE08>, 0.302536231884058], ['4', 0.46107784431137727, <__main__.TrieNode object at 0x0000023F906CEE08>, 0.302536231884058], ['6', 0.2934131736526946, <__main__.TrieNode object at 0x0000023F906CEE08>, 0.302536231884058], ['5', 1.0, <__main__.TrieNode object at 0x0000023F8DCE3BC8>, 0.08876811594202899]] \n",
      "\n",
      "symb:2, 4, 6 state:279 -> children:[['#', 1.0, <__main__.TrieNode object at 0x0000023F906CEE08>, 0.28205128205128205], ['7', 1.0, <__main__.TrieNode object at 0x0000023F906CE348>, 0.02564102564102564], ['0', 1.0, <__main__.TrieNode object at 0x0000023F8E57AF48>, 0.14102564102564102], ['1', 1.0, <__main__.TrieNode object at 0x0000023F8DBD2A48>, 0.08974358974358974], ['6', 1.0, <__main__.TrieNode object at 0x0000023F70C95DC8>, 0.07692307692307693], ['8', 1.0, <__main__.TrieNode object at 0x0000023F70C9C0C8>, 0.16666666666666666], ['5', 1.0, <__main__.TrieNode object at 0x0000023F9E73E908>, 0.07692307692307693], ['3', 1.0, <__main__.TrieNode object at 0x0000023F8D4AF308>, 0.05128205128205128], ['9', 1.0, <__main__.TrieNode object at 0x0000023F8DCDA0C8>, 0.05128205128205128], ['2', 1.0, <__main__.TrieNode object at 0x0000023F8D48CB88>, 0.02564102564102564], ['4', 1.0, <__main__.TrieNode object at 0x0000023FB2CBBA88>, 0.01282051282051282]] \n",
      "\n",
      "symb:27 state:280 -> children:[['#', 1.0, <__main__.TrieNode object at 0x0000023F906CE348>, 1.0]] \n",
      "\n",
      "symb:20 state:287 -> children:[['#', 1.0, <__main__.TrieNode object at 0x0000023F8E57AF48>, 0.9090909090909091], ['5', 1.0, <__main__.TrieNode object at 0x0000023F9E261508>, 0.09090909090909091]] \n",
      "\n",
      "symb:605 state:376 -> children:[['#', 1.0, <__main__.TrieNode object at 0x0000023F9E261508>, 1.0]] \n",
      "\n",
      "symb:21 state:302 -> children:[['#', 1.0, <__main__.TrieNode object at 0x0000023F8DBD2A48>, 1.0]] \n",
      "\n",
      "symb:26, 9 state:303 -> children:[['#', 1.0, <__main__.TrieNode object at 0x0000023F70C95DC8>, 0.7], ['8', 1.0, <__main__.TrieNode object at 0x0000023F8D4AF548>, 0.1], ['5', 1.0, <__main__.TrieNode object at 0x0000023F8D48C248>, 0.1], ['6', 1.0, <__main__.TrieNode object at 0x0000023F80DDE5C8>, 0.1]] \n",
      "\n",
      "symb:268 state:347 -> children:[['#', 1.0, <__main__.TrieNode object at 0x0000023F8D4AF548>, 1.0]] \n",
      "\n",
      "symb:465 state:373 -> children:[['#', 1.0, <__main__.TrieNode object at 0x0000023F8D48C248>, 1.0]] \n",
      "\n",
      "symb:596 state:377 -> children:[['#', 1.0, <__main__.TrieNode object at 0x0000023F80DDE5C8>, 1.0]] \n",
      "\n",
      "symb:28 state:306 -> children:[['#', 1.0, <__main__.TrieNode object at 0x0000023F70C9C0C8>, 0.9230769230769231], ['3', 1.0, <__main__.TrieNode object at 0x0000023F8D48C688>, 0.07692307692307693]] \n",
      "\n",
      "symb:483 state:372 -> children:[['#', 1.0, <__main__.TrieNode object at 0x0000023F8D48C688>, 1.0]] \n",
      "\n",
      "symb:25 state:312 -> children:[['#', 1.0, <__main__.TrieNode object at 0x0000023F9E73E908>, 1.0]] \n",
      "\n",
      "symb:23 state:345 -> children:[['#', 1.0, <__main__.TrieNode object at 0x0000023F8D4AF308>, 1.0]] \n",
      "\n",
      "symb:29 state:351 -> children:[['#', 1.0, <__main__.TrieNode object at 0x0000023F8DCDA0C8>, 1.0]] \n",
      "\n",
      "symb:42 state:374 -> children:[['#', 1.0, <__main__.TrieNode object at 0x0000023F8D48CB88>, 1.0]] \n",
      "\n",
      "symb:64 state:375 -> children:[['#', 1.0, <__main__.TrieNode object at 0x0000023FB2CBBA88>, 1.0]] \n",
      "\n",
      "symb:5 state:291 -> children:[['#', 1.0, <__main__.TrieNode object at 0x0000023F8DCE3BC8>, 0.10204081632653061], ['6', 1.0, <__main__.TrieNode object at 0x0000023F8DCE38C8>, 0.08163265306122448], ['0', 1.0, <__main__.TrieNode object at 0x0000023F815C45C8>, 0.12244897959183673], ['2', 1.0, <__main__.TrieNode object at 0x0000023F4C4B9988>, 0.08163265306122448], ['8', 1.0, <__main__.TrieNode object at 0x0000023F9E74E488>, 0.32653061224489793], ['9', 1.0, <__main__.TrieNode object at 0x0000023F70C95DC8>, 0.10204081632653061], ['3', 1.0, <__main__.TrieNode object at 0x0000023F4C4B8B08>, 0.10204081632653061], ['4', 1.0, <__main__.TrieNode object at 0x0000023F714926C8>, 0.061224489795918366], ['1', 1.0, <__main__.TrieNode object at 0x0000023FA60FE948>, 0.02040816326530612]] \n",
      "\n",
      "symb:56 state:292 -> children:[['#', 1.0, <__main__.TrieNode object at 0x0000023F8DCE38C8>, 1.0]] \n",
      "\n",
      "symb:50 state:301 -> children:[['#', 1.0, <__main__.TrieNode object at 0x0000023F815C45C8>, 0.8333333333333334], ['4', 1.0, <__main__.TrieNode object at 0x0000023F8ABA4288>, 0.16666666666666666]] \n",
      "\n",
      "symb:504 state:364 -> children:[['#', 1.0, <__main__.TrieNode object at 0x0000023F8ABA4288>, 1.0]] \n",
      "\n",
      "symb:52 state:304 -> children:[['#', 1.0, <__main__.TrieNode object at 0x0000023F4C4B9988>, 1.0]] \n",
      "\n",
      "symb:58 state:307 -> children:[['#', 1.0, <__main__.TrieNode object at 0x0000023F9E74E488>, 0.8125], ['0', 1.0, <__main__.TrieNode object at 0x0000023F2D077F48>, 0.0625], ['2', 1.0, <__main__.TrieNode object at 0x0000023F70D7A0C8>, 0.0625], ['3', 1.0, <__main__.TrieNode object at 0x0000023F8D48CD88>, 0.0625]] \n",
      "\n",
      "symb:580 state:308 -> children:[['#', 1.0, <__main__.TrieNode object at 0x0000023F2D077F48>, 1.0]] \n",
      "\n",
      "symb:582 state:360 -> children:[['#', 1.0, <__main__.TrieNode object at 0x0000023F70D7A0C8>, 1.0]] \n",
      "\n",
      "symb:1583 state:371 -> children:[['#', 1.0, <__main__.TrieNode object at 0x0000023F8D48CD88>, 1.0]] \n",
      "\n",
      "symb:53 state:324 -> children:[['#', 1.0, <__main__.TrieNode object at 0x0000023F4C4B8B08>, 0.6], ['8', 1.0, <__main__.TrieNode object at 0x0000023F906C1988>, 0.4]] \n",
      "\n",
      "symb:54 state:342 -> children:[['#', 1.0, <__main__.TrieNode object at 0x0000023F714926C8>, 1.0]] \n",
      "\n",
      "symb:51 state:363 -> children:[['#', 1.0, <__main__.TrieNode object at 0x0000023FA60FE948>, 1.0]] \n",
      "\n",
      "====================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfa_2.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'9': 0.04528985507246377}\n"
     ]
    }
   ],
   "source": [
    "s = dfa_2.prediction(np.array([4, 4, 2, 5]), 1)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'9': 0.0444104134762634}\n"
     ]
    }
   ],
   "source": [
    "s = dfa.prediction(np.array([4, 4, 2, 5]), 1)\n",
    "print(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
